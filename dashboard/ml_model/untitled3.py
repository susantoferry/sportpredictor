# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qaWPOvbcA05lk27r2zSZexOt_lA9PTeV
"""

# !pip install tensorflow==2.13.0
# !pip install keras==2.13.1
# !pip install transformers

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import transformers
from nltk.stem import WordNetLemmatizer
import plotly.express as px
from tqdm.notebook import tqdm
import regex as re
from keras import backend as K

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv('/content/drive/MyDrive/UOW/mbti.csv')
data.head()

#Check if TPU is available
# use_tpu = True
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    strategy = tf.distribute.MirroredStrategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)

px.pie(data,names='type',title='Distribution of personality types',hole=0.3)

data['type'].value_counts()

def clean_text(data):
    data_length=[]
    lemmatizer=WordNetLemmatizer()
    cleaned_text=[]
    for sentence in tqdm(data.posts):
        sentence=sentence.lower()

        #removing links from text data
        sentence=re.sub('https?://[^\s<>"]+|www\.[^\s<>"]+',' ',sentence)

        #removing other symbols
        sentence=re.sub('[^0-9a-z]',' ',sentence)


        data_length.append(len(sentence.split()))
        cleaned_text.append(sentence)
    return cleaned_text

data.posts = clean_text(data)
data

#Split dataset
posts = data['posts'].values
labels =  data['type'].values
train_data, test_data = train_test_split(data, random_state=0, test_size=0.2)

train_size = len(train_data)
test_size = len(test_data)
train_size, test_size

#Initialize Bert tokenizer and masks
from transformers import BertTokenizer
from keras.utils import pad_sequences

bert_model_name = 'bert-base-uncased'

tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)
MAX_LEN = 512

def tokenize_sentences(sentences, tokenizer, max_seq_len = MAX_LEN):
    tokenized_sentences = []
    truncation_strategy = 'only_first'

    for sentence in tqdm(sentences):
        tokenized_sentence = tokenizer.encode(
                            sentence,                  # Sentence to encode.
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = max_seq_len,  # Truncate all sentences.
                            truncation=truncation_strategy
                    )

        tokenized_sentences.append(tokenized_sentence)

    return tokenized_sentences

train_input_ids = tokenize_sentences(train_data['posts'], tokenizer, MAX_LEN)
train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")

test_input_ids = tokenize_sentences(test_data['posts'], tokenizer, MAX_LEN)
test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")

BATCH_SIZE=32
NR_EPOCHS=20

def create_model():
    input_word_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32,
                                           name="input_word_ids")
    bert_layer = transformers.TFBertModel.from_pretrained('bert-base-uncased')
    bert_outputs = bert_layer(input_word_ids)[0]
    pred = tf.keras.layers.Dense(16, activation='softmax')(bert_outputs[:,0,:])

    model = tf.keras.models.Model(inputs=input_word_ids, outputs=pred)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(
    learning_rate=0.00002), metrics=['accuracy'])
    return model

use_tpu = True
if use_tpu:
    # Create distribution strategy
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)

    # Create model
    with strategy.scope():
        model = create_model()
else:
    model = create_model()

model.summary()

types = np.unique(data.type.values)

def get_type_index(string):
    return list(types).index(string)

train_data['type_index'] = data['type'].apply(get_type_index)
train_data

one_hot_labels = tf.keras.utils.to_categorical(train_data.type_index.values, num_classes=16)

from keras.callbacks import EarlyStopping

early_stopping_cb = EarlyStopping(
    monitor='val_loss', # Monitoring the validation loss, the model will stop training if the validation loss is started increasing
    # min_delta=0, # Minimum acceptable threshold for monitor value
    patience=5, # Number of epoch with no improvement after which training will be stopped
    verbose=1
    # mode='auto', # auto means, it will setto minimum in case of loss, it will set to max in case of accuracy
    # baseline=None,
    # restore_best_weights=True # restores the best weight
)

# model.fit(np.array(train_input_ids), one_hot_labels, verbose = 1, epochs = NR_EPOCHS, batch_size = BATCH_SIZE,  callbacks = [tf.keras.callbacks.EarlyStopping(patience = 5)])
model.fit(
    np.array(train_input_ids), one_hot_labels,
    verbose = 1,
    epochs = NR_EPOCHS,
    batch_size = BATCH_SIZE,
    callbacks = [early_stopping_cb])

test_data['type_index'] = data['type'].apply(get_type_index)
test_data

test_labels = tf.keras.utils.to_categorical(test_data.type_index.values, num_classes=16)

score = model.evaluate(np.array(test_input_ids), test_labels)
print('Test loss: {}'.format(score[0]))
print('Test accuracy: {}'.format(score[1]))

cols = data['type'].unique()
cols = cols.tolist()

colnames = ['sentence']
colnames = colnames+cols

df_predict = pd.DataFrame(columns = colnames)
sentence = "Hello World"

df_predict.loc[0, 'sentence'] = sentence

sentence_inputs = tokenize_sentences(df_predict['sentence'], tokenizer, MAX_LEN)
sentence_inputs = pad_sequences(sentence_inputs, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")
prediction = model.predict(np.array(sentence_inputs))
df_predict.loc[0, cols] = prediction

df_predict

df_predict.loc[0, cols]

a = df_predict.loc[0, cols].to_dict()

sorted_data = dict(sorted(a.items(), key=lambda a: a[1]))

sorted_data

sentence_inputs1 = tokenize_sentences(sentence, tokenizer, MAX_LEN)
sentence_inputs1 = pad_sequences(sentence_inputs1, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")
prediction = model.predict(np.array(sentence_inputs1))
# df_predict.loc[0, cols] = prediction

# df_predict

prediction

data['type'].unique()

sentence_inputs1 = tokenize_sentences(sentence, tokenizer, MAX_LEN)

sentence_inputs1

model.save('/content/drive/MyDrive/UOW/bert_uncased_model_new.h5')

